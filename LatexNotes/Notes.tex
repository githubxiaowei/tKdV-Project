\documentclass[12pt]{article}

% PACKAGES
\usepackage{graphicx, amsmath, amssymb, amsfonts, mathtools, mathrsfs, color}
\usepackage{comment, enumerate, tabularx}
\usepackage{natbib, hyperref, url}
\usepackage[margin=1in]{geometry}
%\usepackage[justification=RaggedRight]{caption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LATEX DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Basic editing
\newcommand{\tocite}{{\color{blue}(to cite)}}
\newcommand{\vsp}[1]{\vspace{#1 pc} \noindent}
\newcommand{\np}{\newpage \noindent}
% Derivatives
\newcommand{\pd}[2]    { \frac{\partial #1} {\partial #2} }
\newcommand{\ppd}[2]  { \frac{\partial^2 #1}{{\partial #2}^2} }
\newcommand{\pdi}[2] { {\partial_#2} #1 }
\newcommand{\td}[2] { \frac{d #1} { d #2 } }
\newcommand{\grad}{\nabla}
\newcommand {\Lap} {\grad^2}
% Vectors and operators
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\mean}[1]{\left< #1 \right>}
\newcommand{\eps}{\varepsilon}

% Experimental parameters
\newcommand{\lamm}{\lambda^{-}}
\newcommand{\lamp}{\lambda^{+}}
\newcommand{\hm}{h^{-}}
\newcommand{\hp}{h^{+}}
\newcommand{\etastd}{\eta_{\text std}}
% Theory for truncated KdV and stat mech
\newcommand{\uhat}{\hat{u}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Real}{\text{Re}}
\newcommand{\Fspace}{\mathscr{F}_{\Lambda}}
\newcommand{\Proj}{P_{\Lambda}}
\newcommand{\sumk}{\sum_{k=1}^{\Lambda}}
\newcommand{\sumn}{\sum_{n=1}^{N}}
\newcommand{\uDir}{u_{\text{Dir}}}
\newcommand{\Gibbs}{\mathcal{G}}
\newcommand{\ampp}{\mathcal{A}}


% Dashed integral
\def\Xint#1{\mathchoice
   {\XXint\displaystyle\textstyle{#1}}%
   {\XXint\textstyle\scriptstyle{#1}}%
   {\XXint\scriptstyle\scriptscriptstyle{#1}}%
   {\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
   \!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$}
     \vcenter{\hbox{$#2#3$}}\kern-.5\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}
\newcommand{\intt}{\dashint}%_0^{2 \pi}}
\newcommand{\dx}{\, dx}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Deterministic and statistical truncated KdV models for anomalous waves induced by abrupt depth change}
\author{}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Experimental Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental parameters}

Some relevant parameters from the experiment are given in the table.

% Table
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{table}%[htbp]
\begin{center}
\caption{Basic experimental parameters and scales.}
\label{exptable}
\begin{tabular}{l l l}
\hline Parameter & Upstream  value \,\, & Downstream value \\
\hline
peak forcing frequency, $f_p$\,\,		& 2 Hz			& N/A \\
depth, $h$						& 12.5 cm			& 3 cm \\
peak wavelength, $\lambda_p$		& 38 cm			& 25 cm \\
peak wavenumber, $k_p$			& 0.17 rad/cm		& 0.25 rad/cm \\
dimensionless depth, $k_p h$\,\,\,		& 2.1			& 0.76 \\
\end{tabular}
\end{center}
\end{table}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% KdV
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The KdV equations}

\subsection{The constant-depth KdV equation in dimensional form}

The constant-depth KdV equation in dimensional form is
\begin{equation}
2 \eta_t + \frac{3 c}{h_0} \eta \eta_{\xi} + \frac{c h_0^2}{3} \eta_{\xi \xi \xi} = 0
\end{equation}
Here, $\eta$ has units of length, $t$ time, $h_0$ is the depth, $c = \sqrt{g h_0}$ is the wave speed, and $\xi = x - ct$. I obtained this equation by starting from Johnson's dimensionless equation (3.28) on page 208 and working backwards to untangle the scales that he had used to non-dimensionalize.
Alternatively, when the first term is expressed as a spatial derivative, the dimensional KdV equation is
\begin{equation}
2 \eta_x + \frac{3}{h_0} \eta \eta_{\xi} + \frac{h_0^2}{3} \eta_{\xi \xi \xi} = 0
\end{equation}
Notice the only difference is the absence of the wavespeed $c$ and the first term is now a spatial derivative.

The important thing to remember is that Johnson uses these characteristic scales for non-dimensionalizing (characteristic scales denoted by a *)
\begin{align}
\eta^* = \eps h_0 , \quad
x^* = \eps^{-1/2} h_0 , \quad
t^* = \eps^{-1/2} h_0/c
\end{align}
where $h_0$ is a characteristic depth (say the upstream depth), $c = \sqrt{g h_0}$ the corresponding wave speed, and $\eps = a / h_0$ is the ratio of wave amplitude to depth. We want to alter Johnson's non-dimensionalization so that $x$ is instead scaled on a characteristic wavelength (and $t$ scaled accordingly). Importantly, we need to account for variable depth.


\subsection{The variable-depth KdV equation}

We start with the variable-depth KdV equation given by Johnson in Eq.(3.149) on page 272 in dimensionless form 
\begin{equation}
\label{varKdVJohnson}
2 H_X + \frac{3}{D^{7/4}} H H_\xi + \frac{1}{3} D^{1/2} H_{\xi \xi \xi} = 0
\end{equation}
All variables above are dimensionless, and by working backwards we can figure out how they relate to the original, dimensional variables. Below, the dimensional quantities are indicated with an overbar wherever there is the possibility of ambiguity
\begin{align}
& H = D^{1/4} \eta = \left( \frac{d}{h_0} \right)^{1/4} \frac{\bar{\eta}}{\eps h_0} \\
& X = \eps x = \frac{\eps^{3/2}}{h_0} \bar{x} \\
& \xi = \frac{\eps^{1/2}}{\sqrt{d h_0}} (\bar{x} - c \bar{t})
\end{align}
Here $d$ is the variable depth (which is dimensional) $c = \sqrt{g d}$ is the local wave speed, and $D = d/h_0$ is the depth ratio. The last relationship above for $\xi$ is valid for the case of two depths (though perhaps not in the general case).

There are a few important things to remember when we reformulate this equation:
\begin{enumerate}
\item We must retain Johnson's choice to keep the combination $D^{1/4} \eta$ together because that quantity matches across the abrupt depth change (the raw surface displacement does not match).
\item We want to normalize the standard deviation (or energy) of $\eta$ and therefore want to keep Johnson's choice to scale surface displacement on $a = \eps h_0$.
\item We have to scale the spatial variable (called $x$ or $\xi$) on a characteristic wavelength, which will be a different value on either side of the abrupt depth change. We have to do this so that the assumption of spatial periodicity will be reasonably satisfied on both sides.
\end{enumerate}
To achieve 3, we can use the characteristic wavelength from shallow water theory,
\begin{equation}
\lambda_c = {c}/{f_c} = \sqrt{g d}/f_c
\end{equation}
where $f_c$ is a characteristic forcing frequency and $c = \sqrt{g d}$ is the local wavespeed. It is tempting to simply set $f_c$ to the peak forcing frequency $f_p$, which would result in the characteristic wavelength $\lambda_c$ being the peak wavelength $\lambda_p$. Perhaps a better choice, though, is to let $\lambda_c$ be an integer multiple of $\lambda_p$ to allow scales larger than the peak wavelength (which are certainly present in the experiments). We therefore choose
\begin{equation}
\lambda_c = M \lambda_p = M \sqrt{gd} / f_p
\end{equation}
where $M$ is an integer (on the order of 10 or so). Notice that Johnson's formulation already incorporates the correct scaling of $\sqrt{d}$ to define $\xi$, which will make things convenient. Thus, the main change we need to make is to introduce the dimensionless variable
\begin{equation}
\zeta = \frac{2\pi}{\lambda_c} (\bar{x} - c \bar{t}) 
= \frac{2 \pi \delta}{M \eps^{1/2}} \, \xi
\end{equation}
where
\begin{equation}
\delta = h_0 / \lambda_0 = f_p \sqrt{h_0/g}
\end{equation}
To clean things up a little, lets also introduce a slightly different long-space coordinate that uses the upstream wavelength $\lambda_0$ as a characteristic scale instead of $h_0$,
\begin{equation}
s = \frac{2 \pi \beta}{M \lambda_0} \bar{x} = 2 \pi \beta M^{-1} \delta \eps^{-3/2} X
\end{equation}
where $\beta$ is a small parameter that scale like $\eps$ or $\delta^2$ for example.

Then Eq.~\eqref{varKdVJohnson} becomes
\begin{equation}
\label{varKdVugly}
2 \beta H_s + 3 \eps D^{-7/4} H H_\zeta
+ \frac{4 \pi^2 \delta^2}{3 M^2} D^{1/2} H_{\zeta \zeta \zeta} = 0
\end{equation}

\subsection{Final formulation of variable-depth KdV}

Equation \eqref{varKdVugly} is essentially in the form we want to use, except that the notation is bad. In particular, we will want to reserve the symbol $H$ for the Hamiltonian. So lets introduce some new variables
\begin{align}
& u = D^{1/4} \, \bar{\eta}/a \\
& x = 2\pi \lambda_c^{-1} (\bar{x} - c \bar{t})  \\
& s = 2 \pi \beta M^{-1} \lambda_0^{-1} \bar{x}
\end{align}
Above, the original physical variables are denoted with a bar. Importantly, $u(x,s)$ is $2\pi$-periodic in $x$ and has normalized energy $\mean{u^2} = 1$.

% Table
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{table}[h]%[htbp]
\begin{center}
\caption{Table of parameters}
\label{paramtable}
\begin{tabular}{l l l l}
\hline \multicolumn{4} { c }{Parameters that are constant throughout the domain} \\
\hline Description & Notation and definition & Units & Value in experiments \\
\hline
Peak forcing frequency		& $f_p$						& frequency	& 2 Hz \\
Characteristic wave amplitude	& $a = \mean{\eta^2}^{1/2}$		& length		& 0.03--0.3 cm \\
Upstream wavelength			& $\lambda_0 = \sqrt{g h_0}/f_p$	& length		& 56 cm \\
Amplitude-to-depth ratio		& $\eps = a / h_0$				& dimensionless	& 0.0024--0.024 \\
Depth-to-wavelength ratio		& $\delta = h_0 / \lambda_0$		& dimensionless		& 0.22 \\
Amplitude parameter			& $\ampp = \eps \delta^{-2} = ag h_0^{-2} f_p^{-2}$	
& dimensionless		& 0.05--0.5\\
Catch-all parameter		& $\mu = 4 \pi^2 / (9 M^2)$			& dimensionless		& 0.27 if $M=4$ \\
\hline \multicolumn{4} { c }{Parameters that change value across ADC} \\
\hline Description & Notation and definition & Units \\
\hline
Local depth			& $d$					& length \\
Local wavespeed		& $c = \sqrt{gd}$			& speed \\
Local peak wavelength	& $\lambda_p = \sqrt{gd}/f_p$	& length \\
Characteristic wavelength	& $\lambda_c = M \lambda_p$	& length \\
Depth ratio			& $D = d/h_0$				& dimensionless
\end{tabular}
\end{center}
\end{table}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%


In Table \ref{paramtable} we provide a list of all the relevant parameters needed to convert these dimensional variables to physical ones. Note that the wavelength $\lambda_0$ is predicted from shallow-water theory and so it differs somewhat from the value in Table \ref{exptable} obtained from the dispersion relation. In terms of the new variables,  Eq.~\eqref{varKdVugly} becomes
\begin{align}
\label{varKdV1}
2 \beta u_s + 3 \eps D^{-7/4} \, u u_x
+ \frac{4 \pi^2 \delta^2}{3 M^2} D^{1/2} \, u_{xxx} = 0
\end{align}

Let's clean it up now a little bit. Let us set $\beta = 3 \delta^2/2$ and introduce the parameters
\begin{align}
\ampp = \eps \delta^{-2} = \frac{ag}{h_0^2 f_p^2} , \qquad
\mu = \frac{4 \pi^2}{9 M^2}
\end{align}
The Eq.~\eqref{varKdV1} becomes
\begin{align}
\label{varKdV}
u_s + \ampp D^{-7/4} \, u u_x
+ \mu D^{1/2} \, u_{xxx} = 0
\end{align}

\subsection{Hamiltonian structure}
Then the Hamiltonian structure is given by
\begin{align}
\label{Hamiltonian}
& H^- = \ampp H_3 - \mu H_2 \\
& H^+ = \ampp D_0^{-7/4} H_3 - \mu D_0^{1/2} H_2
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Mathematical Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mathematical preliminaries}

\subsection{Truncated Fourier series}

We will be dealing with the truncated KdV equation (TKDV). 
Consider a real-valued function $u(x): [0, 2\pi] \to \RR$ given by a truncated Fourier series
\begin{equation}
u(x) = \sum_{k= -\Lambda}^{\Lambda} \uhat_k e^{i k x}	\qquad \text{for } 0 \le x \le 2\pi
\end{equation}
where
\begin{equation}
\uhat_k = \intt u(x) e^{-i k x} \dx
\end{equation}
Above, and throughout the paper, we use the notation
\begin{equation}
\intt \cdot \, \dx  = \frac{1}{2\pi} \int_0^{2 \pi} \cdot \, \dx
\end{equation}
We will be dealing with zero-momentum functions
\begin{equation}
M = \intt u(x) \dx = 0
\end{equation}
Let $\Fspace$ denote the space of all zero-momentum functions that can be represented by the above truncated Fourier Series (with maximum wavenumber $\Lambda$).

Since any element $u \in \Fspace$ is real valued and has zero momentum, it can be written as
\begin{equation}
u(x) = \sumk \uhat_k e^{i k x} + \uhat_k^{*} e^{-i k x} 
= 2 \Real \sumk \uhat_k e^{i k x}
\end{equation}
This implies that $\dim(\Fspace) = \Lambda$ as a complex vector space (or the dimension of the equivalent real vector space is $2\Lambda$).
For any $u \in \Fspace$, we introduce the energy
\begin{equation}
E = \frac{1}{2} \intt \Proj(u^2) \dx = \sumk \abs{\uhat_k}^2
\end{equation}
where $\Proj$ is the projection onto $\Fspace$.
In order to define the Hamiltonian for TKDV, we introduce the following components
\begin{equation}
H_3 = \frac{1}{6} \intt \Proj(u^3) \dx \qquad H_2 = \frac{1}{2} \intt \Proj(u_x^2) \dx
\end{equation}
We note that $H_3$ can be written explicitly in Fourier space as \cite{abramov2003hamiltonian} 
\begin{equation}
\label{H3dirsum}
H_3 = \frac{1}{6} \sum_{\substack{ k_1 + k_2 + k_3 = 0 \\ \abs{k_1}, \abs{k_2}, \abs{k_3} \le \Lambda}}\uhat_{k_1} \uhat_{k_2} \uhat_{k_3}
\end{equation}

\subsection{Some useful estimates/bounds for $H_2$ and $H_3$}

Since we examine the statistical mechanics of TKDV, it is valuable to have some a priori bounds and estimates for the components of the Hamiltonian, $H_2$ and $H_3$. In this section, we frequently appeal to the idea of an equipartition of energy among all the modes. For energy $E_0$, if the equipartition holds, then
\begin{equation}
\abs{u_k} = \sqrt{\frac{E_0}{\Lambda}} \qquad \text{for } k=1,\cdots,\Lambda
\end{equation}
We might expect equipartition of energy to hold, for example, in the case of long-time averages or ensemble averages (appealing to ergodicity).

\subsubsection{Expected value of $H_2$ given $E=E_0$}

First, we consider $H_2$. In Fourier space, $u_x$ is represented as
\begin{equation}
u_x(x) = 2 \Real \sumk i k \uhat_k e^{i k x}
\end{equation}
Thus, for equipartitioned energy, we have
\begin{equation}
H_2 = \sumk \abs{k \uhat_k}^2 = \frac{E_0}{\Lambda} \sumk k^2 = \frac{E_0}{6} (\Lambda+1)(2 \Lambda+1) \sim \frac{E_0 \Lambda^2}{3} \text{for } \Lambda \gg 1
\end{equation}
The above provides a useful estimate for the expected value of $H_2$ (to test the numerics for example).

\subsubsection{Upper bound on $H_3$ given $E=E_0$}

Next, consider $H_3$. By the symmetry of the cubic form, the expected value of $H_3$ is zero. However, to sample from Gibbs measures with non-vanishing inverse temperatures using an acceptance/rejection based algorithm, we will need to estimate the maximum allowed value of $H_3$ on the hypersphere of fixed energy $E = E_0$ so that the acceptance rate can be normalized. Note that the set $E=E_0$ is compact and therefore a maximum is guaranteed to exist. A reasonable guess for function that maximizes $H_3$ is something approximating a Dirac delta function, which naturally motivates the Dirichlet kernel
\begin{equation}
\label{Dkern}
\uDir(x) = \sqrt{\frac{E_0}{\Lambda}}\sumk e^{i k x}
\end{equation}
We can compute $H_3(\uDir)$ by using the direct summation formula \eqref{H3dirsum}. Since $\uhat_k = \sqrt{E_0/\Lambda}$, each term in this sum takes the value $(E_0/\Lambda)^{3/2}$, and so we simply need to count the number of terms present in this sum. A careful counting (being sure to dismiss any terms in which one of $k_1, k_2, k_3$ is zero or one of these terms exceeds $\Lambda$) gives
\begin{equation}
\label{H3Dir}
H_3(\uDir) = \frac{1}{2} E_0^{3/2} (\Lambda^{1/2} - \Lambda^{-1/2})
\end{equation}
Certainly, it must be that
\begin{equation}
\max H_3 \ge H_3(\uDir) 
\end{equation}
My conjecture is that
\begin{equation}
\max H_3 = H_3(\uDir) 
\end{equation}
or at least that this relationship holds asymptotically for $\Lambda \gg 1$.
Preliminary numerical tests seem to support this conjecture for tested values $\Lambda = 6, 8, 10, 20$. In fact, the maximum of $H_3$ encountered over about $10^6$ samples is usually significantly smaller than the value predicted in \eqref{H3Dir}. However, for $\Lambda = 4$, my tests encounter values of $H_3$ slightly larger than the predicted value.

Maximizing $H_3$ is perhaps more straightforward in physical space, where we discretize the interval $[0, 2 \pi)$ with 
\begin{equation}
x_n = \frac{2 \pi (n-1)}{N} \quad \text{for } n=1,\cdots,N
\end{equation}
where
\begin{equation}
N = 2 \Lambda \text{ or  } 2 \Lambda +1
\end{equation}
Both are valid choices for using standard real FFT packages.
%$x_n = 2 \pi (n-1)/N$ for $n=1,\cdots,N$, and $N$ is either $2 \Lambda$ or $2 \Lambda + 1$ (both are valid choices for using standard real FFT packages). 
In physical space, we have
\begin{equation}
\label{H3phys}
H_3 = \frac{1}{6} \intt \Proj (u^3) \dx \approx \frac{1}{6N} \sumn u_n^3
\end{equation}
The second relationship is only an approximation because de-aliasing would be required to make the right-hand side exactly equal to \eqref{H3dirsum}. Nonetheless, let us proceed with the estimate. For fixed energy, $E=E_0$, we require
\begin{equation}
E_0 = \intt \Proj(u^2) \dx = \frac{1}{N} \sumn u_n^2
\end{equation}
(I believe the above holds with no approximation being made due to the discrete Parseval identity) 

Clearly, a maximizer of the right-hand-side of \eqref{H3phys} subject to $M=0$ and fixed $E=E_0$ is an approximate Dirac delta function --- i.e. a function with one extreme positive value, and the rest equal negative values,
\begin{align}
& w_1 = \sqrt{2 E_0 (N-1)}		\\
& w_n = -\sqrt{\frac{2 E_0}{N-1}}	\qquad \text{for } n \ne 1
\end{align}
This function is a maximizer because any slight modification (subject to keeping $M=0$ and $E=E_0$) would reduce the value of $H_3$.
Calculating the discrete Fourier transform of this function directly gives
\begin{align}
& \hat{w}_0 = 0	\\
& \hat{w}_k = \sqrt{\frac{2 E_0}{N-1}} \qquad \text{for } k \ne 1
\end{align}
Notice that if we choose $N = 2\Lambda + 1$, then our delta-function in physical space corresponds exactly to the Dirichlet kernel, $w(x) = \uDir(x)$. Now, if we calculate $H_3(w)$ purely in physical space using the approximation in \eqref{H3phys}, we get
\begin{equation}
H_3(w) = \frac{\sqrt{2} E_0^{3/2}}{3} \left( \frac{N-2}{\sqrt{N-1}} \right)
\end{equation}
Taking $N = 2\Lambda + 1$ gives
\begin{equation}
\label{H3physest}
H_3(w) = \frac{2 E_0^{3/2}}{3} \left( \Lambda^{1/2} - \Lambda^{-1/2} \right)
\end{equation}
So while \eqref{H3Dir} and \eqref{H3physest} take very similar forms, they differ by a multiplicative factor: 1/2 in Eq.~\eqref{H3Dir} versus 2/3 in Eq.~\eqref{H3physest}. Therefore, I suppose the error made by failing to de-alias in Eq.~\eqref{H3phys} is not negligible.

So the impasse here as follows: It is straightforward to maximize the approximate form of $H_3$ in physical space (the right-hand-side of \eqref{H3phys}), and justifying this maximization can be made completely rigorous in my mind. However, the approximation made by failing to di-alias is apparently not that small. On the other hand, we can calculate $H_3$ exactly in spectral space, but it is not clear how to maximize $H_3$ subject to the constraints --- I can only make a guess at the maximizer. To overcome this issue, one might naturally want to use the de-aliased form of $H_3$ in physical space (by taking $N = 4 \Lambda$ for example), but the problem here is that all discretized functions are not permissible. Only functions that correspond to a Fourier series truncated at wavenumber $\Lambda$ are permissible.

\np
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Numerical Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical methods}

Here we describe the statistical numerical methods. The deterministic simulations of TKDV are described elsewhere. Below the microstate refers to the spectrum of displacement $\{ \uhat_k \}_{k=1}^{\Lambda}$.

\subsection{Sampling from microcanonical energy distribution}
\label{sec_microcan}

We first consider sampling microstates from a microcanonical energy distribution with $E=E_0$ fixed and $M=0$ as always,
\begin{equation}
\Gibbs^0 = d\nu \propto \delta(E-E_0) \delta(M)
\end{equation}
This can be considered a canonical distribution in the Hamiltonian with zero inverse temperature (hence the notation $\Gibbs^0$). At times, it is easier to denote the distribution $d\nu$, so we will use both interchangeably. Note this is simply the uniform distribution on the hyper-sphere of $E=E_0$.

To sample from this distribution, we use the following algorithm
\begin{enumerate}
\item Sample each $\uhat_k$ for $k=1,\cdots,\Lambda$ from a normal distribution $N(0,1)$.
\item Compute $E = \sumk \abs{\uhat_k}^2$
\item Normalize each component by multiplying by $\sqrt{E_0/E}$.
\end{enumerate}
This algorithm was proven to converge to the measure $d\nu$ in \cite{abramov2003hamiltonian}. Throughout this paper, we normalize the energy for the sampling distribution to unity. This is the reason why $E_0$ appears in the Hamiltonian in \eqref{Hamiltonian}. Hence, we will always be sampling from
\begin{equation}
\Gibbs^0 \propto \delta(E-1) \delta(M)
\end{equation}

\subsection{Acceptance/rejection algorithm to sample from a mixed microcanonical/canonical distribution.}
\label{sec_accrej}

Now consider the problem of sampling from a distribution that is microcanonical in energy and canonical in the Hamiltonian
\begin{equation}
\Gibbs^{\pm} \propto \delta(E-1) \delta(M) e^{-\theta^{\pm} H^{\pm}}
\end{equation}
Here, the Hamiltonian can be either the upstream or downstream, $H = H^-$ or $H^+$ and the inverse temperature $\theta$ can also take different values upstream and downstream, $\theta^-$ and $\theta^+$. To sample from this distribution, we use the following acceptance/rejection algorithm.
\begin{enumerate}
\item Sample a large number of microstates from $\Gibbs^0$.
\item For each, compute $H_3$ and $H_2$. Note: this is the most expensive step in the algorithm. We will therefore store all the values of $H_3$ and $H_2$ so they need not be recomputed.
\item For each sample, compute $H^+$ and $H^-$.
\item Assign a provisional acceptance rate as $e^{- \theta H}$ (for either upstream or downstream). The only issue is this acceptance rate is not yet normalized.
\item Find the maximum of the acceptance rate over all samples $m = \max{e^{-\theta H}}$.
\item Use this value to normalize the acceptance rates, $e^{- \theta H}/m$.
\item Decide to accept or reject each microstate by picking a uniform variable $v \in [0,1]$ and testing if $e^{- \theta H}/m < v$.
\item Store the microstates, as well as the values of $H_3$ and $H_2$, that have been accepted.
\end{enumerate}
The above is a conceptual description of the algorithm. In practice, it is implemented in a way to be computational efficient. In particular, the most expensive step is computing $H_3$ and $H_2$ for a large number of microstates sampled from the microcanonical distribution. We therefore perform this step as precomputation, and save the values of $H_3$ and $H_2$. Afterwards, we can use this master list to perform the acceptance/rejection steps for a number of inverse temperatures, $\theta$, as well as values of $E_0$ and $D_0$.

\subsection{Enforcing the statistical matching condition across ADC}
\label{sec_match}

Importantly, we must enforce the statistical matching condition at the abrupt depth change
\begin{equation}
\label{statmatch}
\mean{H^+}_{\Gibbs^-} = \mean{H^+}_{\Gibbs^+}
\end{equation}
We regard the upstream inverse temperature $\theta^-$ as given, and the downstream value $\theta^+$ as the main unknown to be solved for.
First, note that the ensemble average is given by the formula
\begin{equation}
\label{meanformula}
\mean{H^+}_{\Gibbs^{\pm}} = \frac{ \sum H^+ \exp(- \theta^{\pm} H^{\pm}) } {\sum \exp(-\theta^{\pm} H^{\pm}) }
\end{equation}
where the sum is taken over the ensemble. We therefore enforcing matching condition \eqref{statmatch} as follows

\begin{enumerate}
\item Sample from a large number of microstates (e.g.~$10^8$) from the microcanonical distribution as described above. Compute and save the values of $H_3$ and $H_2$ for each microstate.
\item For prescribed values of $\theta^-$, $E_0$, and $D_0$, compute the value $\mean{H^+}_{\Gibbs^-}$ using \eqref{meanformula}.
\item Define a function to compute the difference $F(\theta^+) = \mean{H^+}_{\Gibbs^-} - \mean{H^+}_{\Gibbs^+}$ for a given $\theta^+$.
\item Find a root of $F$ (e.g.~using the secant method) to determine the value of $\theta^+$ that satisfies \eqref{statmatch}.
\end{enumerate}
Once the value of $\theta^+$ is determined, we sample from the mixed canonical/microcanonical distributions using the algorithm described in the previous section. Note that in practice, we perform step 1 of generating a large number of microsamples only once, and then performing the rest of the algorithm for several different $\theta^-$ values, all using the same list of $H_3$ and $H_2$ values.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminary results}

We show some preliminary results below. First, Figs.~\ref{Hist1}--\ref{Hist3} are histograms of the upstream and downstream Hamiltonians, $H^-$ and $H^+$, respectively, for various values of inverse temperature $\theta^{\pm}$, energy $E_0$, and depth ratio $D_0$. These histograms are made in a very simply way, namely by sampling from the microcanonical distribution (as described in Section \ref{sec_microcan}) and then multiplying by the appropriate factor $\exp(-\theta^{\pm} H^{\pm})$ (and normalizing). There is nothing complex at all in generating these histograms (not even an acceptance/rejection algorithm) and they are only made to give us some initial idea of what is going on.

In these tests, I chose the default values $\Lambda = 10$, $E_0 = 1$, and $D_0 = 0.6$ ($\lambda$ is set to unity throughout). Fig.~\ref{Hist1} shows the upstream and downstream histograms generated with these default values for the inverse temperatures $\theta^{\pm} = 0, \pm 0.3$. First, notice that $\theta$ can shift the mean of $H$ significantly, with negative $\theta$ shifting the mean closer to zero. Second, notice that the downstream histogram has a mean much closer to zero, and this histogram is more concentrated.

In figure \ref{Hist2} we vary the depth ratio. The left shows $D_0 = 0.6$ as before, and the right shows $D_0 = 0.4$. Notice a slight change in the depth ratio can alter the histograms significantly. In figure \ref{Hist3} we vary the energy $E_0$ from the default value of 1 (left) to the value 4. Notice that a significant change in $E_0$ only causes slight changes in the histograms.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{Hist1}
\caption{Left: Sampling $H^-$ from the incoming Gibbs distribution $G^-$ with three different inverse temperatures $\theta^-$ (see legend). Right: Sampling the $H^+$ from an outgoing distribution $G^+$ with the same inverse temperatures.
In these tests, $\Lambda = 10$, $E_0 = 1$, $\lambda = 1$, $D_0 = 0.6$, and the number of samples is $1 \times 10^7$.}
\label{Hist1}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{Hist2}
\caption{Sensitivity to changes in $D_0$.
Left: Downstream histogram with $D_0 = 0.6$ (same as in Fig.~\ref{Hist1}). Right: Same with $D_0 = 0.4$. Notice the histogram changes significantly.
}
\label{Hist2}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{Hist3}
\caption{Sensitivity to changes in the energy $E_0$.
Left: Downstream histogram with $E_0 = 1$ (same as in Fig.~\ref{Hist1}). Right: Same with $E_0 = 4$. Notice a slight change in the histogram.}
\label{Hist3}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Next, in Figs.~\ref{microup0}--\ref{microdn3}, we examine various aspects of the microstates under given Gibbs distributions with possibly non-zero inverse temperatures. For this task, we need to use the acceptance/rejection algorithm described in Section \ref{sec_accrej}. In these tests, we again fix $\Lambda = 10$.
 
 First, Fig.~\ref{microup0} shows features of the microstate with zero inverse temperature (i.e.~the microcanonical distribution). The left panel shows a histogram of the displacement $u$ (in physical space). Notice the histogram is symmetric and resembles a Gaussian distribution. The right panel shows the $\mean{\abs{\uhat_k}}$ versus wavenumber $k$, which illustrates how the energy is distributed amongst the modes. Notice that the energy is evenly distributed amongst modes in this case, i.e.~a equidistribution of energy.
 
 In Fig.~\ref{microup3} we show the same plots but with $\theta^- = -0.3$. The histogram of $u$ is shown on the left. While this histogram still appears nearly symmetric, close examination reveals a slight positive skewness. The right shows $\mean{\abs{\uhat_k}}$. Notice that, in this case of negative inverse temperature, the spectrum is decidedly tilted, with less energy present in the higher modes.
 
 Figure \ref{microdn3} shows the same plots for the {\em downstream} distributions with $\theta^+ = -0.3$. Here, we have not enforced the statistical matching condition \eqref{statmatch}; we simply chose a value of $\theta^+$. First, notice that the histogram of $u$ shows a definite positive skewness. This observation is consistent with the main finding in the experiments! Second, the spectrum is tilted as before, but the tilt is not as strong as it is upstream. Both of these observations are consistent with the weights of $H_3$ and $H_2$. Downstream, the weights strongly favor $H_3$. Therefore, to have a better chance of being selected from the distribution $\Gibbs^{+}$, it is more advantageous to have large skewness of $u$ than it is to have small variance of $u_x$.
  
  %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{microup0}
\caption{Sampling the upstream microstates with $\theta^- = 0$ (and $E_0 = 4$). Left: histogram of the surface displacement, $u^-$. Right: ensemble average of the spectrum, $\mean{\abs{\uhat_k}}$. Notice symmetric distribution of the displacement and the equipartition of energy amongst modes.}
\label{microup0}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{microup3}
\caption{The upstream microstates with $\theta^- = -0.3$ (and $E_0 = 4$ as before). Notice the distinct tilt to the spectrum due to $\theta^- < 0$.}
\label{microup3}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
  %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}[p]%[htbp]
\begin{center}
\includegraphics[width = 0.8 \textwidth]{microdn3}
\caption{Examination of the downstream microstates with $\theta^+ = -0.3$ and $D_0 = 0.5$ ($E_0 = 4$ as before). Notice the skewness of $u$! The spectrum is tilted, though not as much as it is upstream.}
\label{microdn3}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
Now, figures \ref{match_theta}--\ref{match_micro} show results obtained by satisfying the matching condition \eqref{statmatch} using the algorithm described in \ref{sec_match}. In these tests, we use $\Lambda = 8$ and the provisional number of samples is $10^8$ (the number of accepted samples will be much lower). We also fix $E_0 = 4$ and $D_0 = 0.5$.

First, Fig.~\ref{match_theta} shows, for given values of $\theta^-$, the corresponding values of $\theta^+$ as determined by the matching condition. For one, $\theta^- = 0$ corresponds to $\theta^+ = 0$ which is a good sanity check. Second, notice that the signs of $\theta^-$ and $\theta^+$ always agree, which is a good thing. The relationship is not a simply linear one, as the curvature is significant.
 
 Next, Fig.~\ref{matchH} shows upstream and downstream histograms of $H$ with the matching condition enforced. The black curve is the histogram of $\Gibbs^-(H^-)$, while the colored curves are histograms of $H+$. The red curve corresponds to $\Gibbs^-(H^+)$ and the blue to $\Gibbs^+(H^+)$. The matching condition says that the means of these two histograms should be the same, which can be verified visually. Notice, the main difference is that the downstream histogram $\Gibbs^+(H^+)$ is more spread out.
 
Figure \ref{match_micro} shows features of the underlying microstates. On the left, we show histograms of the displacement $u$, as we did in Figs.~\ref{microup0}--\ref{microdn3}, but this time {\em we have enforced the matching condition} \eqref{statmatch}. Notice the upstream $u$-histogram is nearly symmetric, while the downstream histogram is noticeable skewed in the positive direction! The right panel shows the ensemble-averaged spectrum $\mean{\abs{\uhat_k}}$. Both show a tilt in the spectrum, with the upstream tilt being more significant than downstream.

We remark that simulations have been performed with larger values of $\Lambda$, for example $\Lambda = 16$ and 20. For the provisional number of samples fixed, increasing $\Lambda$ does not drastically increase the computational time. However, the main downside to increasing $\Lambda$ is that the acceptance rate for the Gibbs distributions is substantially smaller, and therefore the statistics are less resolved.


%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}%[htbp]
\begin{center}
\includegraphics[width = 0.55 \textwidth]{match_theta}
\caption{For given values of $\theta^-$, we compute $\theta^+$ to satisfy the matching condition.}
\label{match_theta}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}%[htbp]
\begin{center}
\includegraphics[width = 0.75 \textwidth]{matchH}
\caption{Histograms of $H^\pm$ under the two Gibbs measures $\Gibbs^{\pm}$ with the matching condition enforced.}
\label{matchH}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}%[htbp]
\begin{center}
\includegraphics[width = 0.95 \textwidth]{match_micro}
\caption{Microstate features with the matching condition enforced. Left: the histogram of displacement $u$ upstream (red) and downstream (blue). Notice the downstream histogram has a definite positive skewness. Right: the ensemble-averaged spectra. Both spectra are tilted, with the upstream tilt being more significant.}
\label{match_micro}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\np
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Big-time results}

Now lets do some more serious runs with larger $\Lambda$. The major challenge is that the acceptance rate plummets as $\Lambda$ grows, meaning that we either have to take many more samples or develop a smarter strategy. For now, we will do the first. To run more samples, I had to modify the organization of the code. I am still exploiting the ability to precompute a list of $H_3$ and $H_2$ values, but I cannot precompute and store the entire list at once because it will not fit in memory. So instead, I take several sweeps each with a manageable number of samples (usually about $10^7$).

Figure \ref{Lambda16Run} shows a run with $\Lambda = 16$, with $10^7$ samples per sweep and 200 sweeps for a total of $2 \times 10^9$ nominal samples. The main parameter to vary is the upstream inverse temperature $\theta^-$. More negative $\theta^-$ gives a more significantly skewed distribution of $u$, but also results in noisier statistics due to the lower acceptance rate. I therefore chose $\theta^- = -0.2$ since it gives a good balance of the two. The top panel of the figure shows $\theta^+$ as a function of $\theta^-$ and the histograms for $\theta^- = -0.2$. The bottom panel shows microstate information, including the pdf of displacement, $u$, and the spectrum. Notice the definite skew of the downstream displacement and the decay of both upstream and downstream spectra. For this run, the upstream acceptance rate was $0.01\%$ and the downstream rate was $0.008\%$. The total CPU time was about 33 hours.

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^%
\begin{figure}%[htbp]
\begin{center}
\includegraphics[width = 0.95 \textwidth]{Lambda16Run}
\caption{Big run with $\Lambda = 16$ and $2 \times 10^9$ nominal samples.}
\label{Lambda16Run}
\end{center}
\end{figure}
 %^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 

\bibliographystyle{plain}
\bibliography{Notesbib}

\end{document}
